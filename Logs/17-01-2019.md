# Summary

## Relu

I have approximated ReLU using Chebyshev polynomials. Tried with both degree 3 and 4 polynomials. Degree 3 is faster computation time, however, degree 4 provides better accuracy.

## Concrete dataset

Began work on a second dataset, the Concrete Compressive Strength Data set. Aim is to predict the compressive strength of concrete given 8 real valued attributes. Using degree 4 relu approximation I achieve a RMSE or 75 on the encrypted data compared to 50 on the unencrypted data. Need to compare these to a baseline such as always predicting the mean value in order to properly evaluate these scores.

## Challenges

* When using a degree 4 approximation it is more likely that a multiplication in the final layer of the network will 'overflow', causing extrememly large or small values. I experienced this problem on just one data point when I first ran the concrete experiments. This multiplication resulted in the largest value overall and it 'overflowed' the encryption parameters. To solve this I used L2 regularization in the network to keep the weight values as small as possible. However, I believe with data that has larger output values this will be an issue. Some solutions:
    * Increase encryption parameters. This would result in more headroom so to speak but would increase the runtime significantly.
    * Scale the output to have zero mean and unit variance. Train the network on this data and then rescale the predictions to their original mean and variance. Has the advantage of not increasing runtime.

* The reliability of the relu approximation relies on the data falling within a reliable range. This is what we have tried to achieve with using batch normalization. The problem is that batch normalization uses a scaling and shifting after normalizing in order to allow this transformation to be the identity function if needed. However, this causes some of the values after the first layer to leave a comfortable range and the magnitude of these values varies depending on the data set. This makes it difficult to decide on a range to fit a chebyshev polynomial to relu, and different fits result in different performances on each different data set. Some possible solutions:
    * Don't shift and scale after batch normalization layer. This stops the above problem but the accuracy of the unencrypted model suffers as a result
    * Using the above idea it may be a good idea to always scale the output to have zero mean and unit variance, in addition to using L2 regularization to keep the weight values small. This would mean I could more reliably say that values will always fall within a smaller range and one relu fit would do for all different data sets.

* Found a bug in the batch normalization code. I believe the Keras function 'get_weights' that returns the weights and parameters for all layers in the model is incorrect. The documentation led me to believe that one of the values returned was the standard deviation of the batch, however, I now believe this is actually the variance. This led to errors as I was squaring the value returned to me in my batch normalization code, believing it was the standard deviation, to obtain the variance.

## Summary & next steps

* Will try scaling output and regularizing weights instead of batch normalization to see if performance improvements result across different data sets improve for one approximation of relu.
* More data sets. Try some classification tasks as well as regression tasks. 
* Run speed and accuracy comparisons